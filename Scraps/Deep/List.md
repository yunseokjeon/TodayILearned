AI 학습 시간과 데이터를 줄여주는 AI는 작년 초에 이미 나왔다.

여전히 사람이 필요하기 때문에 자가 학습이 시작되는 특이점은 아니지만, 사람의 개입이 줄어드는 속도를 재면 특이점까지의 디데이를 계산할 수 있지 않을까.
[Researchers Build AI That Builds AI](https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/)

[오전 12:37 · 2023년 1월 21일](https://twitter.com/seodam_hst/status/1616459940671164420)

<hr>

When Knyazev and his colleagues came upon the graph hypernetwork idea, they realized they could build upon it. In their new paper, the team shows how to use GHNs not just to find the best architecture from some set of samples, but also to predict the parameters for the best network such that it performs well in an absolute sense. And in situations where the best is not good enough, the network can be trained further using gradient descent.
[Researchers Build AI That Builds AI](https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/)

<hr>

[Parameter Prediction for Unseen Deep Architectures](https://arxiv.org/abs/2110.13100)

<hr>

This CS324-LLM course has a really nice set of notes on large language models.

You can also find lots of key material to read.

https://stanford-cs324.github.io/winter2022/

[마지막으로 수정됨 오전 12:27 · 2023년 1월 12일](https://twitter.com/omarsar0/status/1613195970874843136)

<hr>

LLM을 위시한 초대형 모델들이 지금이야 개비싸지만 한 5년 정도 뒤면 훨씬 저렴하고 강력해질텐데 (약 50-100배 정도 생각) 그 시점이 되면 확실히 뭔가 변곡점이 올 것 같다는 인상이 있음

[오전 1:46 · 2023년 1월 30일](https://twitter.com/summerlight00/status/1619738849391050752)

트랜스포머급 돌파구가 한 두어개 더 나오면 진짜 일반 지능도 불가능은 아닐거 같은데...

[오전 1:50 · 2023년 1월 30일](https://twitter.com/summerlight00/status/1619739810150907905)

<hr>