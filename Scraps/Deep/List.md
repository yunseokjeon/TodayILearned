AI 학습 시간과 데이터를 줄여주는 AI는 작년 초에 이미 나왔다.

여전히 사람이 필요하기 때문에 자가 학습이 시작되는 특이점은 아니지만, 사람의 개입이 줄어드는 속도를 재면 특이점까지의 디데이를 계산할 수 있지 않을까.
[Researchers Build AI That Builds AI](https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/)

[오전 12:37 · 2023년 1월 21일](https://twitter.com/seodam_hst/status/1616459940671164420)

<hr>

When Knyazev and his colleagues came upon the graph hypernetwork idea, they realized they could build upon it. In their new paper, the team shows how to use GHNs not just to find the best architecture from some set of samples, but also to predict the parameters for the best network such that it performs well in an absolute sense. And in situations where the best is not good enough, the network can be trained further using gradient descent.
[Researchers Build AI That Builds AI](https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/)

<hr>

[Parameter Prediction for Unseen Deep Architectures](https://arxiv.org/abs/2110.13100)

<hr>

This CS324-LLM course has a really nice set of notes on large language models.

You can also find lots of key material to read.

https://stanford-cs324.github.io/winter2022/

[마지막으로 수정됨 오전 12:27 · 2023년 1월 12일](https://twitter.com/omarsar0/status/1613195970874843136)

<hr>

LLM을 위시한 초대형 모델들이 지금이야 개비싸지만 한 5년 정도 뒤면 훨씬 저렴하고 강력해질텐데 (약 50-100배 정도 생각) 그 시점이 되면 확실히 뭔가 변곡점이 올 것 같다는 인상이 있음

[오전 1:46 · 2023년 1월 30일](https://twitter.com/summerlight00/status/1619738849391050752)

트랜스포머급 돌파구가 한 두어개 더 나오면 진짜 일반 지능도 불가능은 아닐거 같은데...

[오전 1:50 · 2023년 1월 30일](https://twitter.com/summerlight00/status/1619739810150907905)

<hr>

[Researchers Discover a More Flexible Approach to Machine Learning](https://www.quantamagazine.org/researchers-discover-a-more-flexible-approach-to-machine-learning-20230207)

<hr>

저는 다른 점이 좀 흥미롭네요. 논문에 따르면 이 검사의  IQ는 평균 5.45, 표준편차 1.98인데, 이것을 흔히 하듯이 평균 100, 표준편차 15로 환산하면 소득 상위 25%의 평균 IQ가 100 정도고, 소득 최상위층의 평균 IQ도 110~115 정도 밖에 안됨.
[오후 6:57 · 2023년 2월 8일](https://twitter.com/aichupanda/status/1623259608231075843)

(1) IQ와 소득에는 상관관계가 있습니다. IQ 검사는 원래 학업 능력과 관련되어 개발되었고, IQ가 높은 사람들은 빨리 배우는 경향이 있습니다. IQ가 높다고 무조건 일을 더 잘하지는 않을 수 있는데, 뭔가를 배워서 해야될 때는 유리합니다.
[오전 10:46 · 2023년 2월 9일](https://twitter.com/aichupanda/status/1623498569067171840)

(2) 그런데 IQ와 소득의 상관관계가 존재하려면, 몇 가지 조건이 필요합니다. 첫째, 교육에 대한 접근이 쉬워야 됩니다. 배울 기회가 없으면 빨리 배우는 능력이 소용 없죠. 둘째, 뭔가를 많이 빨리 배우면 돈을 잘 벌 수 있는 직업들이 많아야 합니다. 세상의 직업이 전부 대학원생 같아도 안됩니다.
[오전 10:50 · 2023년 2월 9일](https://twitter.com/aichupanda/status/1623499597191745538)

(3) 가만히 생각해보면 이 두 가지가 모두 성립하기란 쉽지 않습니다. 인류 역사 전체를 통틀어 지난 100년 간 소수의 국가에서나 가능했던 일입니다. 다음 100년에 대해 과감한 전망을 해보자면 "뭔가를 빨리 배우는 능력"의 중요성은 AI의 발전으로 인해 퇴색될거라고 생각합니다.
[오전 10:54 · 2023년 2월 9일](https://twitter.com/aichupanda/status/1623500612376883200)

<hr>

[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

[챗GPT는 어떻게 학습되었을까 - Human Feedback Reinforcement Learning (RLHF)](https://littlefoxdiary.tistory.com/111)

[GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)

<hr>

간단한 예시만으로 언어모델이 외부의 툴을 사용할 수 있게 학습시킬 수 있답니다. 대규모 언어모델은 텍스트 생성은 그럴싸하게 잘하지만 어이없게도 단순한 산수 계산을 틀리기도 하는데 이런 방법을 쓰면 언어모델이 계산기를 사용해서 정확한 계산 결과를 내게 할 수도 있습니다.

[오전 10:47 · 2023년 2월 10일](https://twitter.com/idgmatrix/status/1623861138873290753)

Toolformer: Language Models Can Teach Themselves to Use Tools

introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction

abs: https://arxiv.org/abs/2302.04761

[오전 10:39 · 2023년 2월 10일](https://twitter.com/_akhaliq/status/1623859135132344320)

<hr>

[Solving a machine-learning mystery](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)

<hr>

[Jinhong Kim](https://www.facebook.com/timfr0g/posts/pfbid02WBkDgSWnKG6fx8GQKSPff8wPbqS4p5som1b553gC671wnjXm5F4kbZzQM2vdEFFxl)
 
https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web

ChatGPT가 웹 텍스트 데이터를 손실압축한 것과 비슷하게 볼 수 있다는 관점에 상당히 동의한다.

이 글에 대해 트위터에서 여러 기술자들이 반박하고 있지만, 이는 '손실압축'이라는 기술적 정의에 더 집중하고 있어서 그렇다는 생각이 좀 든다. 대중을 상대로 하는 글쓰기에서 이정도 비유는 허용 가능한 범위다. 진짜로 무엇이 '손실'되는가?에 초점을 두고 보면 지적하는 바가 조금 더 드러난다고 생각한다.

내가 생각하기에, 진짜로 조금씩 손실되는 건 '원본성'과 '원본성'을 뒷받침 하는 '신뢰'다. ChatGPT가 프롬프트의 결과물을 인간 평가자가 매우 여러번 필터링/평가하면서 만들어낸 모델이라는 건 공개된 논문등으로 추측이 가능하다. 특별히 이것 이외에 매우 색다르고 특별한 방법론이 제시되진 않았으므로, 여기서 ChatGPT 모델의 일반화 능력과 품질을 유지하려면 계속 평가자의 개입이 필요하다는 것도 예상 가능하다.

여기서 딜레마가 발생한다. AI는 결국 사람의 개입을 최소화하고 '사람같은 무언가'를 만들기 위해서 만드는 것이다. 그런데 모델을 만들기 위해 평가자가 평가를 더 많이 계속 해야한다면 그 목표가 완전히 달성이 안된다. (물론 예전보다 비용을 크게 줄인다는 의의는 있지만) 그렇다고 어느 순간 평가자가 평가를 중단하면 ChatGPT 모델의 품질을 현재로서는 담보하기 어려울 것으로 보인다.

결국 신뢰 및 원본성이 '손실'되는 것을 벌충하는 사람의 활동이 있다는 이야기다. 학습 데이터 제공이든, 레이블링이든, 결과물의 평가든. 그런데 이 모델이 성공적이라면 너도 나도 쓸테니, 웹에 그런 '생성 콘텐츠'가 계속 늘어날테고 모델의 품질을 유지하기 위한 학습 데이터 제공과 레이블링이 점점 더 어려워질 수 있다. 웹에 생성된 콘텐츠가 매우 많아진다면, 창작자들은 새로운 원본을 만들어내려는 의욕이 떨어질 것이고, 창작의 속도는 줄어들 수 있다. 그런 비관적 시나리오를 두고 'JPEG의 디지털 풍화'에 비유했다고 본다. 물론 미래가 항상 비관적이지는 않겠지만, 그렇다고 항상 낙관적이리라는 보장은 없으니, 이런 지적은 가치가 있을 것이다.

<hr>

번역기 DeepL의 성능에 깜짝 놀라는데, 딥러닝 학습 초기에 레이블링이 잘 된 데이터를 넣어주었을텐데, 앞으로도 계속 양질의 데이터를 넣어줘야 DeepL의 성능이 유지되는 건가요? 아니면 어느 시점이 되면 알아서 학습하나요?

[괴골  2023년 02월 12일 (일)](https://ask.fm/cfr0g/answers/172599513834)

번역 태스크에 관련해서 '알아서 학습'하는 방법은 아직 없습니다. 강화학습이 되는 태스크 범위가 그리 넓지 않습니다.

<hr>
